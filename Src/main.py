# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1L7BM8ieVAdzk0li9NOVb7BsgqelwcyiO
"""

#import drive for accessing dataset.
import pandas as pd

# Mount Google Drive (if needed)
from google.colab import drive
drive.mount('/content/drive')

#Load the pre-trained Word2Vec model for representing nodes into vector representation.
import gensim
import gensim.downloader as api
wv = api.load('word2vec-google-news-300')

#importing required modules for generating Interaction Graphs
from LoadInput import LoadInput
from ConversationForest import ConversationForest
from InteractionNetwork import InteractionNetwork

#importing all required libraries
import numpy as np
import networkx as nx
import csv
import matplotlib.pyplot as plt
from gensim.models import Word2Vec
import pandas as pd
#define deepwalk for generating embeddings by performing random walk and word2vec.
class DeepWalk:
    def __init__(self, graph: nx.Graph):
        self.graph = graph

    def train(self, window_size=5, embedding_size=128, walk_length=30, num_walks=150):
        walks = self.generate_random_walks(walk_length, num_walks)
        model = self.learn_embeddings(walks, embedding_size)
        embeddings = self.extract_node_embeddings(model)
        return embeddings
#for multiple walks.
    def generate_random_walks(self, walk_length, num_walks):
        walks = []
        for _ in range(num_walks):
            for node in self.graph.nodes():
                if not self.graph.neighbors(node):  # Skip nodes with no neighbors
                    continue
                walk = self.random_walk(node, walk_length)
                walks.append(walk)
        return walks
#for single walk generation.
    def random_walk(self, start_node, walk_length):
        walk = [start_node]
        for _ in range(walk_length - 1):
            current_node = walk[-1]
            neighbors = list(self.graph.neighbors(current_node))
            if neighbors:
                weights = [self.graph[current_node][neighbor]['weight'] for neighbor in neighbors]
                probabilities = [weight / sum(weights) for weight in weights]
                next_node = np.random.choice(neighbors, p=probabilities)
                walk.append(next_node)
            else:
                break
        return walk
#Getting embeddings by using word2vec
    def learn_embeddings(self, walks, embedding_size):
        model = Word2Vec(walks, vector_size=embedding_size, window=5, min_count=0, sg=1, workers=1)
        return model

    def extract_node_embeddings(self, model):
        embeddings = {}
        for node in self.graph.nodes():
            embeddings[node] = model.wv[str(node)]
        return embeddings

#constructing graph based on embeddings.
class DeepWalkReduction:
    def __init__(self, graph: nx.Graph):
        self.graph = graph

    def runDeepWalkReduction(self) -> nx.Graph:
        embeddings = self.generateDeepWalkEmbeddings()
        newGraph = self.constructGraphFromEmbeddings(embeddings)
        return newGraph
    def generateDeepWalkEmbeddings(self):
        deepwalk = DeepWalk(self.graph)
        embeddings = deepwalk.train(embedding_size=128, walk_length=10, num_walks=50)
        return embeddings
    def constructGraphFromEmbeddings(self, embeddings):
        newGraph = nx.Graph()
        for node in embeddings:
            newGraph.add_node(node)
        for u, neighbors in self.graph.adjacency():
            for v in neighbors:
                if u in embeddings and v in embeddings:
                    newGraph.add_edge(u, v)
        return newGraph

    def showGraph(self, graph):
        pos = nx.spring_layout(graph, seed=3113794652)
        nx.draw(graph, pos, with_labels=True, node_color='lightblue', edge_color='gray', font_size=8)
        plt.show()

import pandas as pd
import numpy as np
import csv

filepaths=["/content/drive/MyDrive/3.xlsx","/content/drive/MyDrive/4.xlsx","/content/drive/MyDrive/5.xlsx","/content/drive/MyDrive/6.xlsx","/content/drive/MyDrive/7.xlsx","/content/drive/MyDrive/8.xlsx","/content/drive/MyDrive/9.xlsx","/content/drive/MyDrive/10.xlsx","/content/drive/MyDrive/11.xlsx","/content/drive/MyDrive/13.xlsx","/content/drive/MyDrive/15.xlsx","/content/drive/MyDrive/16.xlsx","/content/drive/MyDrive/17.xlsx","/content/drive/MyDrive/18.xlsx","/content/drive/MyDrive/20.xlsx"]

#for iteration of multiple files
for i in filepaths:
  loadInput = LoadInput(i)
  loadInput.loadDataFromAllSheets()
  conversationForest = ConversationForest(loadInput)
  conversationForest.buildConversationTrees()
  trees = conversationForest.getAllConversationTrees()

  all_embeddings = {}

  for key in trees.keys():
    tree = trees[key]
    iNetwork = InteractionNetwork(tree,[] )
    #iNetwork.showInteractionNetwork( )
    deepwalk_reduction = DeepWalkReduction(iNetwork.graph)
    new_graph = deepwalk_reduction.runDeepWalkReduction()
    deepwalk_reduction.showGraph(new_graph)
    deepwalk_embeddings = deepwalk_reduction.generateDeepWalkEmbeddings()

    # Add the embeddings to the all_embeddings dictionary
    all_embeddings.update(deepwalk_embeddings)
    #all_embeddings[key] = deepwalk_embeddings
  print(all_embeddings)
  print(len(all_embeddings))

  #creation of csv file for node embeddings
  def dict_to_csv(dictionary, filename):
    with open(filename, 'w', newline='') as file:
        writer = csv.writer(file)
        writer.writerow(["Author id", "embeddings"])  # Write column headers

        for key, value in dictionary.items():
            writer.writerow([key, value])
  dict_to_csv( all_embeddings, "data.csv")

  #csv file for labels
  Lable_dict={}
  Sorted_label_dict={}
  df = pd.read_excel(i,sheet_name="post")
  # print(df[['discussion_id','author_id']])
  def getAuthorId(id):
    for i in range(len(df)):
      if(df['post_id'][i]==id):
        if(np.isnan(df['parent_post_id'][i])):
          return df['discussion_stance_id'][i]
        else:
          return getAuthorId(df['parent_post_id'][i])
  def getLable(parent_post_id,i):
    if(np.isnan(parent_post_id)):
      if df['author_id'][i] in Lable_dict :
        Lable_dict[df['author_id'][i]].append(df['discussion_stance_id'][i])
      else:
        Lable_dict[df['author_id'][i]] = [df['discussion_stance_id'][i]]
    else:
      x = getAuthorId(parent_post_id)
      if df['author_id'][i] in Lable_dict :
        Lable_dict[df['author_id'][i]].append(x)
      else:
        Lable_dict[df['author_id'][i]] = [x]

  def dict_to_csv(dictionary, filename):
    with open(filename, 'w', newline='') as file:
        writer = csv.writer(file)
        writer.writerow(["Author id", "label"])  # Write column headers

        for key, value in dictionary.items():
            writer.writerow([key, value])

  def findmax(list1):
    Highest_count = list1[0]
    for i in list1:
      if list1.count(Highest_count) < list1.count(i):
        Highest_count = i
      elif list1.count(Highest_count) == list1.count(i):
        Highest_count = max(Highest_count,i)
    return Highest_count

  for i in range(len(df)):
    getLable(df['parent_post_id'][i],i)

  for i in Lable_dict:
    Sorted_label_dict[i] = findmax(Lable_dict[i])
    # print(i ,"-->",Lable_dict[i],"-->",findmax(Lable_dict[i]))

  dict_to_csv(Sorted_label_dict, "dictionary.csv")
  #print(len(dictionary))

  #creating merged file
  csv.field_size_limit(100000000)
  def merge_csv_with_embeddings(labels_file, embeddings_file, merged_file):
    # Read the labels CSV file and store its contents in a dictionary
    labels_dict = {}
    with open(labels_file, 'r') as file:
        reader = csv.DictReader(file)
        for row in reader:
            author_id = row['Author id']
            label = row['label']
            labels_dict[author_id] = {'label': label}

    # Read the embeddings CSV file and update the dictionary with the embeddings
    with open(embeddings_file, 'r') as file:
        reader = csv.DictReader(file)
        for row in reader:
            author_id = row['Author id']
            embeddings = row['embeddings']
            if author_id in labels_dict:
                labels_dict[author_id]['embeddings'] = embeddings

   # Write the merged data to a new CSV file
    fieldnames = ['author_id', 'label', 'embeddings']
    with open(merged_file, 'w', newline='') as file:
        writer = csv.DictWriter(file, fieldnames=fieldnames)
        writer.writeheader()
        for author_id, data in labels_dict.items():
            data['author_id'] = author_id
            writer.writerow(data)

  labels_file = '/content/dictionary.csv'
  embeddings_file = '/content/data.csv'
  merge_csv_with_embeddings(labels_file, embeddings_file, "merged_file.csv")


  #CNN MODEL
  import math
  import tensorflow as tf
  import numpy as np
  from sklearn.model_selection import train_test_split
  from sklearn.metrics import classification_report

  # Load the merged data from the CSV file
  merged_file = '/content/merged_file.csv'

  author_ids = []
  embeddings = []
  labels = []

  with open(merged_file, 'r') as file:
    reader = csv.DictReader(file)
    for row in reader:
        author_ids.append(row['author_id'])
        # Process the embeddings string and convert it to a numeric format
        embeddings_str = row['embeddings']
        embeddings_str = embeddings_str.replace('\n', '').replace('[', '').replace(']', '')
        embeddings_values = embeddings_str.split()
        embeddings_values = [float(value) for value in embeddings_values]
        embeddings.append(embeddings_values)

        label_value = float(row['label'])
        label = int(label_value) if not math.isnan(label_value) else 0
        labels.append(label)

  # Convert author_ids and labels to numpy arrays
  author_ids = np.array(author_ids)
  embeddings = np.array(embeddings)
  labels = np.array(labels)

  # Determine the number of classes
  num_classes = np.max(labels) + 1

  # Find the maximum length of embeddings
  max_length = max(len(embedding) for embedding in embeddings)

  # Pad the embeddings to have the same length
  embeddings_padded = tf.keras.preprocessing.sequence.pad_sequences(embeddings, maxlen=max_length)

  # Split the data into training and testing sets
  embeddings_train, embeddings_test, labels_train, labels_test = train_test_split(embeddings_padded, labels, test_size=0.2, random_state=42)

  # Define the CNN Architecture
  embedding_dim = embeddings_padded.shape[-1]

  model = tf.keras.models.Sequential([
      #tf.keras.layers.Reshape((embedding_dim, 1), input_shape=(embedding_dim,)),
      tf.keras.layers.Conv1D(64, kernel_size=5, activation='relu'),
      tf.keras.layers.MaxPooling1D(pool_size=2),
      #tf.keras.layers.Flatten(),
      tf.keras.layers.Dense(128, activation='relu'),
      tf.keras.layers.Dense(num_classes, activation='softmax')
  ])

  # Compile the Model
  model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

  # Train the Model
  model.fit(embeddings_train, labels_train, epochs=10, batch_size=32, validation_data=(embeddings_test, labels_test))

  # Evaluate the Model
  loss, accuracy = model.evaluate(embeddings_test, labels_test, verbose=0)
  print("Test Loss:", loss)
  print("Test Accuracy:", accuracy)

  # Make Predictions
  predictions = model.predict(embeddings_test)

  # Convert predictions to class labels
  predicted_labels = np.argmax(predictions, axis=1)

  # Generate classification report
  report = classification_report(labels_test, predicted_labels)
  print(report)